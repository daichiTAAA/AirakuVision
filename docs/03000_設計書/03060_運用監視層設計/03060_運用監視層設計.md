# 運用監視層設計書：統合オブザーバビリティプラットフォーム

## 1. 目的
本書は、映像・音声ベース作業分析自動化システムの運用監視層について、システム全体の統合的なオブザーバビリティを実現する設計を定義する。OpenTelemetry、Kafka、Prometheus、Grafana、Jaegerによる統一的な監視・分析基盤の構築を目的とする。

## 2. 適用範囲
- 装着型デバイス層、配信・記録層、業務サービス層、データマネジメント層、分析・可視化層からのテレメトリ収集
- OpenTelemetry による統一テレメトリ収集と Kafka による分散メッセージング
- Prometheus によるメトリクス管理とアラート
- Grafana による統合ダッシュボードと可視化
- Jaeger による分散トレーシング
- システム全体の運用・保守・障害対応

## 3. 要件マッピング
| 要件カテゴリ                        | 対応内容                                                   |
| ----------------------------------- | ---------------------------------------------------------- |
| 機能 (F-052, F-053)                 | システム監視・通知機能、フェイルオーバー制御               |
| 非機能 (N-004, N-005, N-006, N-017) | 高可用性監視、障害検知・復旧、データ整合性監視、拡張性対応 |
| 運用 (O-001, O-002, O-003, O-004)   | 24/365 監視体制、変更管理、構成管理、バックアップ・DR      |

## 4. システム構成概要

### 4.1 統合オブザーバビリティアーキテクチャ
```
[各システム層のOpenTelemetry Collector] 
    ↓ OTLP Protocol
[Kafka Cluster (テレメトリハブ)]
    ↓ Consumer Groups
[Prometheus] [Jaeger] [Grafana]
    ↓ Alertmanager
[運用チーム通知システム]
```

### 4.2 主要コンポーネント
| コンポーネント        | 役割                                                                                   | 冗長構成                                               |
| --------------------- | -------------------------------------------------------------------------------------- | ------------------------------------------------------ |
| Kafka Cluster         | テレメトリデータのハブとして、全システム層からのメトリクス・トレース・ログを集約・配信 | 3ノード構成、レプリケーション係数3                     |
| OpenTelemetry Gateway | 各層から送信されるOTLPデータを受信し、Kafkaトピックに振り分け                          | 2ノード構成、ロードバランサー配下                      |
| Prometheus            | Kafkaからメトリクスデータを取得し時系列データベースとして保存、アラート評価            | 2ノード構成（Primary-Secondary）                       |
| Grafana               | 統合ダッシュボードによる可視化、PrometheusおよびJaegerデータソース統合                 | 2ノード構成、共有データベース                          |
| Jaeger                | 分散トレーシングデータの保存・検索・可視化、サービス間依存関係分析                     | All-in-One → Production構成（Collector、Query、Agent） |
| Alertmanager          | アラートルーティング・集約・通知、エスカレーションポリシー管理                         | 2ノード構成、Gossipクラスター                          |

## 5. データフローとメッセージング設計

### 5.1 Kafkaトピック設計
| トピック名          | 用途                       | パーティション数 | レプリケーション |
| ------------------- | -------------------------- | ---------------- | ---------------- |
| `telemetry-metrics` | 全層からのメトリクスデータ | 12               | 3                |
| `telemetry-traces`  | 分散トレーシングスパン     | 8                | 3                |
| `telemetry-logs`    | 構造化ログデータ           | 6                | 3                |
| `alerts-raw`        | 生アラートイベント         | 4                | 3                |
| `alerts-processed`  | 処理済みアラート通知       | 2                | 3                |

### 5.2 OpenTelemetryデータ変換とルーティング
- **装着型デバイス層**: デバイス状態、ネットワーク遅延、バッテリーレベル → `telemetry-metrics`
- **配信・記録層**: ストリーム統計、録画処理メトリクス、ストレージ容量 → `telemetry-metrics`, `telemetry-traces`
- **業務サービス層**: API応答時間、作業イベント処理、データベース接続 → `telemetry-metrics`, `telemetry-traces`
- **データマネジメント層**: ETL処理状況、バッチジョブ実行時間 → `telemetry-metrics`
- **分析・可視化層**: ユーザーセッション、画面表示時間、分析処理負荷 → `telemetry-metrics`, `telemetry-traces`

### 5.3 メッセージフォーマット
- **プロトコル**: OTLP over Kafka with Protobuf
- **パーティショニング**: `device_id`、`service_name`、`trace_id` による負荷分散
- **保持期間**: メトリクス 30日、トレース 7日、ログ 14日
- **圧縮**: Snappy圧縮でネットワーク効率化

## 6. 監視・アラート設計

### 6.1 階層別監視項目
#### 装着型デバイス層
- デバイス接続状況、バッテリー残量、配信品質、音声認識精度
- 閾値: バッテリー残量20%以下、配信遅延1秒超過、接続断3分継続

#### 配信・記録層  
- ストリーム遅延、録画成功率、MinIO容量、ADLS転送状況
- 閾値: 遅延1秒超過60秒継続、録画失敗率5%超過、ストレージ80%超過

#### 業務サービス層
- API応答時間、データベース接続プール、作業イベント処理遅延
- 閾値: API応答時間3秒超過、DB接続プール90%超過、イベント処理遅延5分

#### データマネジメント層
- ETLジョブ実行状況、データ整合性チェック、レポート生成時間
- 閾値: ETLジョブ失敗、データ不整合検出、レポート生成24時間遅延

#### 分析・可視化層
- ダッシュボード応答性、ユーザーセッション数、分析処理負荷
- 閾値: 画面表示時間10秒超過、同時ユーザー数100超過、CPU使用率90%

### 6.2 アラートエスカレーションポリシー
| 優先度   | 対象                           | 通知方法                | エスカレーション           |
| -------- | ------------------------------ | ----------------------- | -------------------------- |
| Critical | システム停止、データ損失リスク | 即座に電話・SMS・Slack  | 5分以内に上位管理者        |
| High     | 性能劣化、部分機能停止         | 15分以内にSlack・メール | 30分以内にオンコール担当者 |
| Medium   | 警告レベル、予防的通知         | 1時間以内にメール       | 4時間以内にチームリーダー  |
| Low      | 情報通知、トレンド異常         | 日次レポートに含める    | エスカレーションなし       |

## 7. ダッシュボードとレポート設計

### 7.1 Grafana統合ダッシュボード構成
#### システム概要ダッシュボード
- 全システム層の稼働状況サマリー
- リアルタイム配信数、録画処理数、作業イベント数
- 直近24時間のアラート履歴とトレンド

#### レイヤー別詳細ダッシュボード
- 各層固有のKPIとメトリクス
- サービス間依存関係の可視化（Jaeger統合）
- パフォーマンス統計とボトルネック分析

#### 運用管理ダッシュボード  
- インフラリソース使用状況（CPU、メモリ、ディスク、ネットワーク）
- Kafka クラスターヘルス、Prometheusクエリ性能
- バックアップ状況、DR準備状況

### 7.2 自動レポート機能
- **日次レポート**: システム稼働率、処理件数サマリー、異常事象一覧
- **週次レポート**: パフォーマンストレンド、リソース使用傾向、改善推奨事項  
- **月次レポート**: SLA達成状況、キャパシティプランニング、保守作業サマリー

## 8. 分散トレーシング設計

### 8.1 Jaegerによるトレース分析
- **エンドツーエンドトレーシング**: 装着型デバイスからの映像配信→録画→メタデータ登録→分析UI表示までの全工程
- **サービス依存関係マップ**: 各層間の呼び出し関係と処理時間の可視化  
- **レイテンシー分析**: P50、P95、P99レイテンシーの監視とSLA管理
- **エラー率追跡**: 各サービス間でのエラー発生箇所の特定と根本原因分析

### 8.2 トレースサンプリング戦略
- **ヘッドベースサンプリング**: 通常運用時は10%サンプリング
- **テールベースサンプリング**: エラーまたは高レイテンシーの場合は100%保持
- **適応的サンプリング**: システム負荷に応じてサンプリング率を動的調整

## 9. 高可用性・障害対応設計

### 9.1 冗長構成
- **Kafka**: 3ノードクラスター、最小同期レプリカ数2
- **Prometheus**: Primary-Secondary構成、データ同期とフェイルオーバー自動化
- **Grafana**: ロードバランサー配下の2ノード、PostgreSQL共有データベース
- **Jaeger**: Collector-Agent-Query分離アーキテクチャ、Elasticsearchクラスター

### 9.2 災害復旧・事業継続
- **データバックアップ**: Kafka データ、Prometheusメトリクス、Grafanaダッシュボード定義の定期バックアップ
- **クロスリージョンレプリケーション**: 主要メトリクスとアラート定義の副次サイト同期
- **緊急時運用手順**: 監視システム自体の障害時の手動監視・通知プロセス

## 10. セキュリティ・プライバシー

### 10.1 認証・認可
- **Kafka**: SASL/SCRAM認証、TLS暗号化、ACLによるトピック別アクセス制御
- **Prometheus**: Basic認証またはOAuth2、スクレイプターゲット別アクセス権限
- **Grafana**: LDAP統合、ロールベースアクセス制御、ダッシュボード共有権限管理
- **Jaeger**: OAuth2認証、トレースデータの機密性分類

### 10.2 データ保護
- **テレメトリデータ匿名化**: 個人識別可能な情報のマスキング・ハッシュ化
- **ログサニタイゼーション**: 機密情報の自動検出・除去
- **アクセス監査**: 監視システムへのアクセスログとレポート機能

## 11. 運用・保守手順

### 11.1 定期保守作業
- **日次**: システムヘルスチェック、アラート状況確認、ダッシュボード確認
- **週次**: Kafkaディスク容量確認、Prometheusデータ保持期間管理
- **月次**: バックアップ検証、DR演習、パフォーマンスレビュー

### 11.2 障害対応手順  
1. **即座対応**: アラート受信から15分以内の初期対応、影響範囲特定
2. **根本原因分析**: JaegerトレースとPrometheusメトリクスによる原因調査
3. **復旧作業**: サービス復旧と回避策実施、ユーザー影響最小化  
4. **事後対応**: インシデントレポート作成、再発防止策検討・実装

### 11.3 変更管理プロセス
- **監視設定変更**: アラート閾値、ダッシュボード定義の変更承認フロー
- **システム更新**: OpenTelemetry、Kafka、Prometheus等のバージョンアップ手順
- **新規監視項目追加**: 新機能・新サービス追加時の監視要件定義と実装

## 12. 性能・拡張性設計

### 12.1 スケーリング戦略
- **水平スケール**: Kafkaパーティション追加、Prometheusシャーディング
- **垂直スケール**: 高負荷時のリソース増強、SSDストレージ高速化
- **自動スケーリング**: Kubernetes HPA/VPAによるGrafana・Jaeger自動拡張

### 12.2 パフォーマンス目標
- **メトリクス取り込み**: 100,000 metrics/second
- **トレース処理**: 10,000 spans/second  
- **ダッシュボード応答**: 平均3秒以内、P95で5秒以内
- **アラート通知遅延**: Critical 30秒以内、High 2分以内

---
本設計に基づき、システム全体の統合的なオブザーバビリティを実現し、安定運用と継続的改善を支援する監視基盤を構築する。
